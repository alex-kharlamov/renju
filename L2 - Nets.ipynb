{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 - Полносвязанная сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Многоклассовая классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы знакомы уже с бинарным линейным классификатором\n",
    "$$y = \\text{sign}(wx).$$\n",
    "Существуют [разные](https://en.wikipedia.org/wiki/Multiclass_classification) подходы к задаче многоклассовой классификции, к примеру [сведение](https://en.wikipedia.org/wiki/Multiclass_classification#Transformation_to_Binary) задачи к бинарной классификации, [модификация модели](https://en.wikipedia.org/wiki/Support_vector_machine#Multiclass_SVM) и т.п. Нам же интересен подход, который применяется в нейронных сетях.\n",
    "\n",
    "Для каждого класса из набора $1 \\dots |C|$ заведем свой вектор весов $w_i$ и уложим это все в матрицу $W$ построчно. Для простоты будем считать, что $w_i$ - строка. Тогда наш классификатор будет выглядеть следующим образом\n",
    "$$(p_1, \\dots, p_{|C|}) = \\text{softmax}(Wx),$$\n",
    "где $p_i$ - вероятность, что объект относится к классу $i$, при этом\n",
    "$$p_i = \\frac{\\exp(w_ix)}{\\sum_j \\exp(w_jx)}.$$\n",
    "Если внимательно присмотреться, то $\\text{softmax}$ является обобщенным вариантом сигмоиды. Для того, чтобы убедиться в этом, достаточно расписать случай для $|C|=2$.\n",
    "\n",
    "Как и для задачи бинарной классификации, обучение можно свести к минимизации эмпирического риска, то есть к оптимизации следующего функционала\n",
    "$$\\arg\\min_W Q(W) = \\arg\\min_W -\\frac{1}{\\mathcal{l}}\\sum_y\\sum_i [y = i] \\cdot \\ln(p_i(W)).$$\n",
    "Очевидно, что сверху написано ни что иное, как максимизация логарифма правдоподобия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "1. Вычислите градиент функции $Q$ (попробуйте провести выкладки для отдельной строки $w_i$).\n",
    "2. Обучите модель с помощью градиентного спуска на выборке [mnist](https://www.kaggle.com/c/digit-recognizer) (вы можете применить свою любимую вариацию метода).\n",
    "3. Вычислите качество на отоженной выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Имеем \\space Q = -\\frac{1}{\\mathcal{l}}\\sum_y\\sum_i [y = i] \\cdot \\ln(p_i(W))$$\n",
    "\n",
    "$$ Q(W) = -[y = i]\\cdot\\log\\frac{exp(W_{i}x)}{\\sum_{j}exp(W_{j}x)} = [y = i]\\cdot(\\log(\\sum_{j}exp(W_{j}x)) - \\log(exp(W_{i}x))) = [y = i]\\cdot(\\log(\\sum_{j}exp(W_{j}x)) - W_{i}x)$$\n",
    "\n",
    "$$Тогда \\space Q'(W_i^k) = x_k\\cdot[y = i]\\cdot(\\frac{\\sum_{j}exp(W_{j}x)'}{\\sum_{j}exp(W_{j}x)} - 1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "def log_progress(sequence, every=None, size=None):\n",
    "    from ipywidgets import IntProgress, HTML, VBox\n",
    "    from IPython.display import display\n",
    "\n",
    "    is_iterator = False\n",
    "    if size is None:\n",
    "        try:\n",
    "            size = len(sequence)\n",
    "        except TypeError:\n",
    "            is_iterator = True\n",
    "    if size is not None:\n",
    "        if every is None:\n",
    "            if size <= 200:\n",
    "                every = 1\n",
    "            else:\n",
    "                every = int(size / 200)     # every 0.5%\n",
    "    else:\n",
    "        assert every is not None, 'sequence is iterator, set every'\n",
    "\n",
    "    if is_iterator:\n",
    "        progress = IntProgress(min=0, max=1, value=1)\n",
    "        progress.bar_style = 'info'\n",
    "    else:\n",
    "        progress = IntProgress(min=0, max=size, value=0)\n",
    "    label = HTML()\n",
    "    box = VBox(children=[label, progress])\n",
    "    display(box)\n",
    "\n",
    "    index = 0\n",
    "    try:\n",
    "        for index, record in enumerate(sequence, 1):\n",
    "            if index == 1 or index % every == 0:\n",
    "                if is_iterator:\n",
    "                    label.value = '{index} / ?'.format(index=index)\n",
    "                else:\n",
    "                    progress.value = index\n",
    "                    label.value = u'{index} / {size}'.format(\n",
    "                        index=index,\n",
    "                        size=size\n",
    "                    )\n",
    "            yield record\n",
    "    except:\n",
    "        progress.bar_style = 'danger'\n",
    "        raise\n",
    "    else:\n",
    "        progress.bar_style = 'success'\n",
    "        progress.value = index\n",
    "        label.value = str(index or '?')\n",
    "        \n",
    "        \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "data_y = data['label']\n",
    "del data['label']\n",
    "\n",
    "data = data.as_matrix()\n",
    "data_y = data_y.tolist()\n",
    "\n",
    "data = StandardScaler().fit_transform(data)\n",
    "\n",
    "temp = []\n",
    "\n",
    "for elem in data_y:\n",
    "    temp_vec = [0 for i in range(10)]\n",
    "    temp_vec[elem] = 1\n",
    "    temp.append(temp_vec)\n",
    "    \n",
    "data_y = np.array(temp)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    data, data_y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dense():\n",
    "    def __init__(self, inp, out):\n",
    "        self.W = np.random.randn(inp, out) * 0.1\n",
    "        self.b = np.zeros(out)\n",
    "        \n",
    "    def get_params_iter(self):\n",
    "        for it in [self.W, self.b]:\n",
    "            for element in it:\n",
    "                yield element\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        return X @ self.W + self.b\n",
    "        \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        der_W = X.T @ output_grad\n",
    "        der_b = np.sum(output_grad, axis=0)\n",
    "        temp = []\n",
    "        for it in [der_W, der_b]:\n",
    "            for element in it:\n",
    "                temp.append(element)\n",
    "        return temp\n",
    "    \n",
    "    def get_input_grad(self, output_grad=None, T=None):\n",
    "        return output_grad @ self.W.T\n",
    "    \n",
    "    \n",
    "class Nonlinearity():\n",
    "    def __init__(self, name = \"sigmoid\"):\n",
    "        self.name = name\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        if self.name == \"relu\":\n",
    "            np.clip(X, 0, np.finfo(X.dtype).max, out=X)\n",
    "            return X\n",
    "        if self.name == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-X))\n",
    "        if self.name == \"identity\":\n",
    "            return X\n",
    "    \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        return []\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        return []\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        if self.name == \"relu\":\n",
    "            def der_relu(x):\n",
    "                if (x < 0).all:\n",
    "                    return 0\n",
    "                else:\n",
    "                    return 1   \n",
    "            result = der_relu(Y)\n",
    "            return np.array(result)\n",
    "        if self.name == \"sigmoid\":\n",
    "            return np.multiply(Y, (1 - Y)) * output_grad\n",
    "        if self.name == \"identity\":\n",
    "            return np.sign(Y)\n",
    "            \n",
    "    \n",
    "class Softmax():\n",
    "    def __init__(self, reg = \"None\", alpha_l1 = 0.1, alpha_l2 = 0.1):\n",
    "        self.reg = reg\n",
    "        self.alpha_l1 = alpha_l1\n",
    "        self.alpha_l2 = alpha_l2\n",
    "        \n",
    "    def get_output(self, X):\n",
    "        return np.exp(X)/np.sum(np.exp(X),axis=1, keepdims=True)\n",
    "    \n",
    "    def get_input_grad(self, Y, T):\n",
    "        if self.reg == \"None\":\n",
    "            return (Y - T) / Y.shape[0]\n",
    "        if self.reg == \"L1\":\n",
    "            return (Y - T) / Y.shape[0] + (self.alpha_l1 * np.sign(X)).sum()\n",
    "        if self.reg == \"L2\":\n",
    "            return (Y - T) / Y.shape[0] + (self.alpha_l2 * 2 * X).sum()\n",
    "        if self.reg == \"both\":\n",
    "            return (Y - T) / Y.shape[0] + (self.alpha_l2 * 2 * X).sum() + (self.alpha_l1 * np.sign(X)).sum()\n",
    "    \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        return []\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        return []\n",
    "    \n",
    "def forward_step(input_samples, layers):\n",
    "    activations = [input_samples]\n",
    "    temp_dat = []\n",
    "    X = input_samples\n",
    "    for layer in layers:\n",
    "        Y = layer.get_output(X)\n",
    "        temp_dat.append(X)\n",
    "        activations.append(Y)\n",
    "        X = activations[-1]\n",
    "    return activations, temp_dat\n",
    "\n",
    "def backward_step(activations, targets, layers):\n",
    "    param_grads = collections.deque()\n",
    "    output_grad = None\n",
    "    for layer in layers[::-1]:\n",
    "        Y = activations.pop()\n",
    "        temp_X = temp_dat.pop()\n",
    "        if output_grad is None:\n",
    "            input_grad = layer.get_input_grad(Y, targets)\n",
    "        else:\n",
    "            input_grad = layer.get_input_grad(Y, output_grad)\n",
    "        X = activations[-1]\n",
    "        grads = layer.get_params_grad(X, output_grad)\n",
    "        param_grads.appendleft(grads)\n",
    "        output_grad = input_grad\n",
    "    return list(param_grads)\n",
    "\n",
    "\n",
    "def batch_iterator(X_train, Y_train, batch_size):\n",
    "    for i in range(int(X_train.shape[0] / batch_size)):\n",
    "        yield [X_train[i * batch_size : i * batch_size + batch_size],\n",
    "               Y_train[i * batch_size : i * batch_size + batch_size]]\n",
    "\n",
    "def update_params(layers, param_grads, learning_rate):\n",
    "    for layer, layer_backprop_grads in zip(layers, param_grads):\n",
    "        for param, grad in zip(layer.get_params_iter(), layer_backprop_grads):\n",
    "            param -= learning_rate * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = []\n",
    "\n",
    "layers.append(Dense(X_train.shape[1], 10))\n",
    "layers.append(Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9928193092346191\n",
      "1.0338892936706543\n",
      "1.016998529434204\n",
      "0.9951286315917969\n",
      "1.1504430770874023\n"
     ]
    }
   ],
   "source": [
    "max_nb_of_iterations = 5\n",
    "learning_rate = 0.1\n",
    "batch_size = 1000\n",
    "import time\n",
    "\n",
    "for iteration in range(max_nb_of_iterations):\n",
    "    begin = time.time()\n",
    "    for X, T in batch_iterator(X_train, Y_train, batch_size): \n",
    "        activations, temp_dat = forward_step(X, layers)\n",
    "        param_grads = backward_step(activations, T, layers, temp_dat)\n",
    "        update_params(layers, param_grads, learning_rate)\n",
    "    print(time.time() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy =  0.891071428571\n"
     ]
    }
   ],
   "source": [
    "y_true = np.argmax(Y_test, axis=1)\n",
    "activations = forward_step(X_test, layers)\n",
    "y_pred = np.argmax(activations[-1][1], axis=1) \n",
    "test_accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "print('The accuracy = ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте кратко каснемся темы регуляризации. Как было рассказано на семинаре, подходы есть разные. Мы же остановимся на модификации оптимизируемого функционала.\n",
    "\n",
    "$$\\arg\\min_W -\\frac{1}{\\mathcal{l}}\\sum_y\\sum_i [y = i] \\cdot \\ln(p_i(W)) + \\lambda_1 L_1(W) + \\lambda_2 L_2(W)$$\n",
    "\n",
    "1. $L_1(W) = sum_{i,j} |w_{i,j}|$ - пытается занулить бесполезные признаки\n",
    "2. $L_2(W) = sum_{i,j} w_{i,j}^2$ - не дает параметрам быть слишком большими."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "1. Как стоит подбирать значения $\\lambda_1$ и $\\lambda_2$?\n",
    "2. Удалось ли улучшить $Q$ на отложенной выборке?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/site-packages/ipykernel/__main__.py:63: RuntimeWarning: overflow encountered in exp\n",
      "/usr/lib/python3.5/site-packages/ipykernel/__main__.py:63: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "accuracy_per_reg = []\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for j in log_progress(range(20)):\n",
    "    for i in log_progress(range(20)):\n",
    "        layers = []\n",
    "\n",
    "        layers.append(Dense(X_train.shape[1], 10))\n",
    "        layers.append(Softmax(reg=\"both\", alpha_l1 = 0.05 * i, alpha_l2 = 0.05 * j))\n",
    "        \n",
    "        xs.append(0.05 * i)\n",
    "        ys.append(0.05 * j)\n",
    "\n",
    "        max_nb_of_iterations = 5\n",
    "        learning_rate = 0.1\n",
    "        batch_size = 1000\n",
    "        import time\n",
    "\n",
    "        for iteration in range(max_nb_of_iterations):\n",
    "            for X, T in batch_iterator(X_train, Y_train, batch_size): \n",
    "                activations, temp_dat = forward_step(X, layers)\n",
    "                param_grads = backward_step(activations, T, layers, temp_dat)\n",
    "                update_params(layers, param_grads, learning_rate)\n",
    "\n",
    "        y_true = np.argmax(Y_test, axis=1)\n",
    "        activations = forward_step(X_test, layers)\n",
    "        y_pred = np.argmax(activations[-1][1], axis=1) \n",
    "        test_accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "        accuracy_per_reg.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f4055124d68>"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFKCAYAAADMuCxnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\np5Kel/Qu8CbwxRx3PQjmFf8DI4b9Iskly08AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4056d92048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.title(\"Regularization parameters\")\n",
    "plt.xlabel(\"L1 alpha\")\n",
    "plt.ylabel(\"L2 alpha\")\n",
    "ax.scatter(xs, ys, accuracy_per_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Регуляризаторы не работают =("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Полносвязанные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно задаться вопросом, а что будет, если поверх модели $Wx$ применить еще одну матрицу, а затем уже $\\text{softmax}$? Ответ следующий, дополнительное умножение на матрицу слева $W_2\\cdot(W_1x)$ ничего не даст, в итоге это та же самая модель $Wx$, где $W=W_2 W_1$. А вот, если между матрицами добавить третий \"слой\", то может получиться очень интерсная штука."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте рассмотрим пример на множестве бинарных функций, $x \\in \\{0, 1\\}^{2}$. Пусть есть модель\n",
    "\n",
    "$$y = \\theta(wx + b),$$\n",
    "\n",
    "где $\\theta$ - функция [Хевисайда](https://en.wikipedia.org/wiki/Heaviside_step_function).\n",
    "\n",
    "**Задание**\n",
    "1. Предложите значения $w$ и $b$, чтобы $y$ реализовала операторы *and*, *or*, *not*.\n",
    "2. Приведите пример булевой функции, которая не может быть представлена в виде $y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. And: $w_1 = 1/2, w_2 = 1/2,  b = -2/3$\n",
    "\n",
    "   Or: $w_1 = 1/2, w_2 = 1/2,  b = -1/3$\n",
    "   \n",
    "   Not: $w = -1,  b = 1/2$\n",
    "    \n",
    "    \n",
    "2. XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте рассмотрим случай посложнее, пусть теперь $x \\in \\{0, 1\\}^{n}$, а\n",
    "$$y = \\theta(W_2 \\cdot \\theta(W_1x + b_1) + b_2),$$\n",
    "где функция $\\theta$ для вектора применяется поэлементно. Кстати, судя по размерности, $W_2$ - вектор, но для общности записан с большой буквы.\n",
    "\n",
    "**Задание**\n",
    "1. Можете ли вы теперь представить вашу функцию в виде $y$?\n",
    "2. Может ли $y$ реализовать произвольную булеву функцию? Знаете ли вы, что такое ДНФ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем виде будем иметь:\n",
    "\n",
    "1. AND : $w_1 = diag(1, 1, ..., 1), w_2 = diag(1, 1, ..., 1), b_1 = diag(-1/2, -1/2, ..., -1/2), b_2 = -n + 1/2$\n",
    "2. OR : $w_1 = diag(1, 1, ..., 1), w_2 = diag(1, 1, ..., 1), b_1 = (0, 0, 0, ..., 0), b_2 = -1/2$\n",
    "\n",
    "Тогда, так как выразимы and, or, not, то выразима и любая функция представимая в виде ДНФ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы получили интуицию, почему сочетание нелинейности и нескольких линейных слоев могут дать поразительный эффект! Очевидно, что эту же идею можно переложить и на задачу классификации в пространстве признаков $\\mathbb{R}^n$.\n",
    "Одним словом, мы изобрели полносвязанную нейронную сеть!\n",
    "\n",
    "Конструкция сети обычно выглядит следующим образом. На вектор признаков $x_1$ сначала применяется линейное преобрзование $W_1$ и некоторая нелинейная функция $\\sigma_1$. На выходе мы получаем вектор $x_2$. К нему может быть применена очередная пара $W_2$ и $\\sigma_2$ и т.д. Для задачи классификации в частности все заканчивается слоем $\\text{softmax}$, которые применяется к выходу $x_{n}$. Теперь, пропустив входной вектор $x_1$ через все эти слои, получим оценку вероятности принадлежности каждому классу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые полезные факты:\n",
    "1. Двуслойная сеть в $\\mathbb{R}^n$ позволяет реализовать произвольный выпуклый многогранник.\n",
    "2. Трехслойная сеть в $\\mathbb{R}^n$ позволяет определить уже не обязательно выпуклую и даже не обязательно связанную область.\n",
    "3. С помощью линейных операций и фиксированной функции $\\sigma$ можно приблизить функцию с коль угодной точностью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось только понять, как все эти слои обучить. Для этого нам понадобиться ввести кое-какие обозначения. Пусть есть некоторая функция, которая на вход принимает вектор $x = (x_1, \\dots, x_n)$ и выдает $y = (y_1, \\dots, y_m)$, то есть размерности могу не совпадать, тогда\n",
    "\n",
    "$$\\frac{dy}{dx} = \\Big[g_{ij} = \\frac{\\partial y_j}{\\partial x_i}\\Big].$$\n",
    "\n",
    "Не нарушая общности можно предположить, что каждый слой это функция $$f_i(x_i, w_i) = x_{i+1}.$$ При этом для обучения в самый конец добавляется слой с вычислением ошибки. Таким образом всю работу такой сети можно обозначить за $Q$, которая принимает $x_1$ и возвращает некоторое значение ошибки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Forward step**\n",
    "\n",
    "Первым делом просто запускаем вычисления по слоям и узнаем все значения $x_i$. Тут все просто и справится даже школьник."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backward step**\n",
    "\n",
    "Рассмотрим последний слой, градиентный шаг для $w_n$ вычисляется очевидным образом $\\frac{dQ}{dw_n}$. При этом на всякий случай вычислим еще и $\\frac{dQ}{dx_{n}}$. \n",
    "\n",
    "Теперь на необходимо посчитать то же самое на слой ниже, но на семинаре мы подробно обсуждали, что \n",
    "$$\\frac{dQ}{dw_{n-1}} = \\frac{dx_{n}}{dw_{n-1}}\\frac{dQ}{dx_{n}} \\text{ и } \\frac{dQ}{dx_{n-1}} = \\frac{dx_{n}}{dx_{n-1}}\\frac{dQ}{dx_{n}}.$$\n",
    "То есть теперь мы вычислили градиент для парамтеров слоем ниже. Повторяя таки образом операции, вычислим $\\frac{dQ}{dw_{n-2}}$ и $\\frac{dQ}{dx_{n-2}}$ и т.д.\n",
    "\n",
    "Теперь, когда градиент найден, мы можем спокойно применить метод градиентного спуска для обучения весов. Стоит отметить, что сети обучают именно таким подходом, при этом для лучшего обучения повсеместно применяется батчинг."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Моменты реализации**\n",
    "\n",
    "Итак, нейросеть удобно реализовывать в виде слоев $y=f(x, w)$, где у каждого слоя есть следующие методы:\n",
    "* $f$ - вычисляет значение слоя для  текущего значения $w$ и вектора $x$.\n",
    "* $dydw$ - метод необходимый для вычисление шага градиентного спуска\n",
    "* $dydx$ - метод необходим для вычисления шага для слоя ниже\n",
    "* $update$ -  обновляет параметр $w$ для слоя на указанную величину\n",
    "\n",
    "Тогда, если есть список слоев и определена функция потерь последним дополнительным слоем, то обучение сводиться к запуска forward вычислений, а затем постепенном обратном распространении ошибки и обновления параметров, то есть шаг $backward$. Будьте аккуратны при реализации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**\n",
    "\n",
    "1. Реализуйте двухслойную сеть, где в качестве нелинейной функции используется [ReLu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks).\n",
    "2. Улучшилось ли качество в сравнении с предыдущей моделью?\n",
    "3. Какова размерность выходного вектора после первого линейного преобразования? Как влияет его размер?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_neurons_1 = 30\n",
    "hidden_neurons_2 = 10  \n",
    "layers = []\n",
    "\n",
    "layers.append(Dense(X_train.shape[1], hidden_neurons_1))\n",
    "layers.append(Nonlinearity(name='relu'))\n",
    "\n",
    "layers.append(Dense(hidden_neurons_1, hidden_neurons_2))\n",
    "layers.append(Nonlinearity(name='relu'))\n",
    "\n",
    "layers.append(Dense(hidden_neurons_2, Y_train.shape[1]))\n",
    "layers.append(Softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, cross_validation, metrics\n",
    "import itertools\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.152688026428223\n",
      "7.0878214836120605\n",
      "7.067523956298828\n",
      "7.124407052993774\n",
      "6.9789135456085205\n"
     ]
    }
   ],
   "source": [
    "max_nb_of_iterations = 5\n",
    "learning_rate = 0.1\n",
    "batch_size = 1000\n",
    "import time\n",
    "\n",
    "for iteration in range(max_nb_of_iterations):\n",
    "    begin = time.time()\n",
    "    for X, T in batch_iterator(X_train, Y_train, batch_size): \n",
    "        activations, temp_dat = forward_step(X, layers)\n",
    "        param_grads = backward_step(activations, T, layers)\n",
    "        update_params(layers, param_grads, learning_rate)\n",
    "    print(time.time() - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy =  0.930714285714\n"
     ]
    }
   ],
   "source": [
    "y_true = np.argmax(Y_test, axis=1)\n",
    "activations = forward_step(X_test, layers)\n",
    "y_pred = np.argmax(activations[-1][1], axis=1) \n",
    "test_accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "print('The accuracy = ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество улучшилось довольно значительно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_10 (Dense)                 (None, 512)           401920      dense_input_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, 512)           0           dense_10[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 512)           262656      activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 512)           0           dense_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 10)            5130        activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 10)            0           dense_12[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 669706\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.214404754218\n",
      "Test accuracy: 0.967023809524\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 20\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=0, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_312 (Dense)                (None, 512)           401920      dense_input_106[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_311 (Activation)      (None, 512)           0           dense_312[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_313 (Dense)                (None, 512)           262656      activation_311[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_312 (Activation)      (None, 512)           0           dense_313[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_314 (Dense)                (None, 10)            5130        activation_312[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_313 (Activation)      (None, 10)            0           dense_314[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 669706\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.216831819047\n",
      "Test accuracy: 0.967976190476\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_315 (Dense)                (None, 512)           401920      dense_input_107[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_314 (Activation)      (None, 512)           0           dense_315[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_316 (Dense)                (None, 512)           262656      activation_314[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_315 (Activation)      (None, 512)           0           dense_316[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_317 (Dense)                (None, 512)           262656      activation_315[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_316 (Activation)      (None, 512)           0           dense_317[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_318 (Dense)                (None, 10)            5130        activation_316[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_317 (Activation)      (None, 10)            0           dense_318[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 932362\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.190033720539\n",
      "Test accuracy: 0.9675\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_319 (Dense)                (None, 512)           401920      dense_input_108[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_318 (Activation)      (None, 512)           0           dense_319[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_320 (Dense)                (None, 512)           262656      activation_318[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_319 (Activation)      (None, 512)           0           dense_320[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_321 (Dense)                (None, 512)           262656      activation_319[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_320 (Activation)      (None, 512)           0           dense_321[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_322 (Dense)                (None, 512)           262656      activation_320[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_321 (Activation)      (None, 512)           0           dense_322[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_323 (Dense)                (None, 10)            5130        activation_321[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_322 (Activation)      (None, 10)            0           dense_323[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1195018\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.163780066165\n",
      "Test accuracy: 0.968095238095\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_324 (Dense)                (None, 512)           401920      dense_input_109[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_323 (Activation)      (None, 512)           0           dense_324[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_325 (Dense)                (None, 512)           262656      activation_323[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_324 (Activation)      (None, 512)           0           dense_325[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_326 (Dense)                (None, 512)           262656      activation_324[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_325 (Activation)      (None, 512)           0           dense_326[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_327 (Dense)                (None, 512)           262656      activation_325[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_326 (Activation)      (None, 512)           0           dense_327[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_328 (Dense)                (None, 512)           262656      activation_326[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_327 (Activation)      (None, 512)           0           dense_328[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_329 (Dense)                (None, 10)            5130        activation_327[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_328 (Activation)      (None, 10)            0           dense_329[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1457674\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.191025911048\n",
      "Test accuracy: 0.969166666667\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_330 (Dense)                (None, 512)           401920      dense_input_110[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_329 (Activation)      (None, 512)           0           dense_330[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_331 (Dense)                (None, 512)           262656      activation_329[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_330 (Activation)      (None, 512)           0           dense_331[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_332 (Dense)                (None, 512)           262656      activation_330[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_331 (Activation)      (None, 512)           0           dense_332[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_333 (Dense)                (None, 512)           262656      activation_331[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_332 (Activation)      (None, 512)           0           dense_333[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_334 (Dense)                (None, 512)           262656      activation_332[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_333 (Activation)      (None, 512)           0           dense_334[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_335 (Dense)                (None, 512)           262656      activation_333[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_334 (Activation)      (None, 512)           0           dense_335[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_336 (Dense)                (None, 10)            5130        activation_334[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_335 (Activation)      (None, 10)            0           dense_336[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1720330\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.237800729193\n",
      "Test accuracy: 0.962619047619\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_337 (Dense)                (None, 512)           401920      dense_input_111[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_336 (Activation)      (None, 512)           0           dense_337[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_338 (Dense)                (None, 512)           262656      activation_336[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_337 (Activation)      (None, 512)           0           dense_338[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_339 (Dense)                (None, 512)           262656      activation_337[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_338 (Activation)      (None, 512)           0           dense_339[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_340 (Dense)                (None, 512)           262656      activation_338[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_339 (Activation)      (None, 512)           0           dense_340[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_341 (Dense)                (None, 512)           262656      activation_339[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_340 (Activation)      (None, 512)           0           dense_341[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_342 (Dense)                (None, 512)           262656      activation_340[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_341 (Activation)      (None, 512)           0           dense_342[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_343 (Dense)                (None, 512)           262656      activation_341[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_342 (Activation)      (None, 512)           0           dense_343[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_344 (Dense)                (None, 10)            5130        activation_342[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_343 (Activation)      (None, 10)            0           dense_344[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1982986\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.213864729976\n",
      "Test accuracy: 0.967738095238\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_345 (Dense)                (None, 512)           401920      dense_input_112[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_344 (Activation)      (None, 512)           0           dense_345[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_346 (Dense)                (None, 512)           262656      activation_344[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_345 (Activation)      (None, 512)           0           dense_346[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_347 (Dense)                (None, 512)           262656      activation_345[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_346 (Activation)      (None, 512)           0           dense_347[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_348 (Dense)                (None, 512)           262656      activation_346[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_347 (Activation)      (None, 512)           0           dense_348[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_349 (Dense)                (None, 512)           262656      activation_347[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_348 (Activation)      (None, 512)           0           dense_349[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_350 (Dense)                (None, 512)           262656      activation_348[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_349 (Activation)      (None, 512)           0           dense_350[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_351 (Dense)                (None, 512)           262656      activation_349[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_350 (Activation)      (None, 512)           0           dense_351[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_352 (Dense)                (None, 512)           262656      activation_350[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_351 (Activation)      (None, 512)           0           dense_352[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_353 (Dense)                (None, 10)            5130        activation_351[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_352 (Activation)      (None, 10)            0           dense_353[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2245642\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.202644583508\n",
      "Test accuracy: 0.96619047619\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_354 (Dense)                (None, 512)           401920      dense_input_113[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_353 (Activation)      (None, 512)           0           dense_354[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_355 (Dense)                (None, 512)           262656      activation_353[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_354 (Activation)      (None, 512)           0           dense_355[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_356 (Dense)                (None, 512)           262656      activation_354[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_355 (Activation)      (None, 512)           0           dense_356[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_357 (Dense)                (None, 512)           262656      activation_355[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_356 (Activation)      (None, 512)           0           dense_357[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_358 (Dense)                (None, 512)           262656      activation_356[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_357 (Activation)      (None, 512)           0           dense_358[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_359 (Dense)                (None, 512)           262656      activation_357[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_358 (Activation)      (None, 512)           0           dense_359[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_360 (Dense)                (None, 512)           262656      activation_358[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_359 (Activation)      (None, 512)           0           dense_360[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_361 (Dense)                (None, 512)           262656      activation_359[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_360 (Activation)      (None, 512)           0           dense_361[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_362 (Dense)                (None, 512)           262656      activation_360[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_361 (Activation)      (None, 512)           0           dense_362[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_363 (Dense)                (None, 10)            5130        activation_361[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_362 (Activation)      (None, 10)            0           dense_363[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2508298\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.220400963936\n",
      "Test accuracy: 0.966785714286\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_364 (Dense)                (None, 512)           401920      dense_input_114[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_363 (Activation)      (None, 512)           0           dense_364[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_365 (Dense)                (None, 512)           262656      activation_363[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_364 (Activation)      (None, 512)           0           dense_365[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_366 (Dense)                (None, 512)           262656      activation_364[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_365 (Activation)      (None, 512)           0           dense_366[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_367 (Dense)                (None, 512)           262656      activation_365[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_366 (Activation)      (None, 512)           0           dense_367[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_368 (Dense)                (None, 512)           262656      activation_366[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_367 (Activation)      (None, 512)           0           dense_368[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_369 (Dense)                (None, 512)           262656      activation_367[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_368 (Activation)      (None, 512)           0           dense_369[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_370 (Dense)                (None, 512)           262656      activation_368[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_369 (Activation)      (None, 512)           0           dense_370[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_371 (Dense)                (None, 512)           262656      activation_369[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_370 (Activation)      (None, 512)           0           dense_371[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_372 (Dense)                (None, 512)           262656      activation_370[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_371 (Activation)      (None, 512)           0           dense_372[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_373 (Dense)                (None, 512)           262656      activation_371[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_372 (Activation)      (None, 512)           0           dense_373[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_374 (Dense)                (None, 10)            5130        activation_372[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_373 (Activation)      (None, 10)            0           dense_374[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 2770954\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.195602353952\n",
      "Test accuracy: 0.964642857143\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 20\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for deep in range(1, 10):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(784,)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    for elem in range(deep):\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, Y_train,\n",
    "                        batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                        verbose=0, validation_data=(X_test, Y_test))\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    accuracy.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, относительно простой модели Softmax качество значительно улучшилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_390 (Dense)                (None, 512)           401920      dense_input_118[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_389 (Activation)      (None, 512)           0           dense_390[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_391 (Dense)                (None, 1)             513         activation_389[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_390 (Activation)      (None, 1)             0           dense_391[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_392 (Dense)                (None, 10)            20          activation_390[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_391 (Activation)      (None, 10)            0           dense_392[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 402453\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 1.49112589768\n",
      "Test accuracy: 0.467380952381\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_393 (Dense)                (None, 512)           401920      dense_input_119[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_392 (Activation)      (None, 512)           0           dense_393[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_394 (Dense)                (None, 51)            26163       activation_392[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_393 (Activation)      (None, 51)            0           dense_394[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_395 (Dense)                (None, 10)            520         activation_393[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_394 (Activation)      (None, 10)            0           dense_395[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 428603\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.21171977703\n",
      "Test accuracy: 0.969761904762\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_396 (Dense)                (None, 512)           401920      dense_input_120[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_395 (Activation)      (None, 512)           0           dense_396[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_397 (Dense)                (None, 101)           51813       activation_395[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_396 (Activation)      (None, 101)           0           dense_397[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_398 (Dense)                (None, 10)            1020        activation_396[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_397 (Activation)      (None, 10)            0           dense_398[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 454753\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.184873057692\n",
      "Test accuracy: 0.970833333333\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_399 (Dense)                (None, 512)           401920      dense_input_121[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_398 (Activation)      (None, 512)           0           dense_399[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_400 (Dense)                (None, 151)           77463       activation_398[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_399 (Activation)      (None, 151)           0           dense_400[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_401 (Dense)                (None, 10)            1520        activation_399[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_400 (Activation)      (None, 10)            0           dense_401[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 480903\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.220646163389\n",
      "Test accuracy: 0.966071428571\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_402 (Dense)                (None, 512)           401920      dense_input_122[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_401 (Activation)      (None, 512)           0           dense_402[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_403 (Dense)                (None, 201)           103113      activation_401[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_402 (Activation)      (None, 201)           0           dense_403[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_404 (Dense)                (None, 10)            2020        activation_402[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_403 (Activation)      (None, 10)            0           dense_404[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 507053\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.302189495597\n",
      "Test accuracy: 0.96\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_405 (Dense)                (None, 512)           401920      dense_input_123[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_404 (Activation)      (None, 512)           0           dense_405[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_406 (Dense)                (None, 251)           128763      activation_404[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_405 (Activation)      (None, 251)           0           dense_406[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_407 (Dense)                (None, 10)            2520        activation_405[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_406 (Activation)      (None, 10)            0           dense_407[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 533203\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.220383038916\n",
      "Test accuracy: 0.969761904762\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_408 (Dense)                (None, 512)           401920      dense_input_124[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_407 (Activation)      (None, 512)           0           dense_408[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_409 (Dense)                (None, 301)           154413      activation_407[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_408 (Activation)      (None, 301)           0           dense_409[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_410 (Dense)                (None, 10)            3020        activation_408[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_409 (Activation)      (None, 10)            0           dense_410[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 559353\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.193750314081\n",
      "Test accuracy: 0.969642857143\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_411 (Dense)                (None, 512)           401920      dense_input_125[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_410 (Activation)      (None, 512)           0           dense_411[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_412 (Dense)                (None, 351)           180063      activation_410[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_411 (Activation)      (None, 351)           0           dense_412[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_413 (Dense)                (None, 10)            3520        activation_411[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_412 (Activation)      (None, 10)            0           dense_413[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 585503\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.208295272562\n",
      "Test accuracy: 0.970476190476\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_414 (Dense)                (None, 512)           401920      dense_input_126[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_413 (Activation)      (None, 512)           0           dense_414[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_415 (Dense)                (None, 401)           205713      activation_413[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_414 (Activation)      (None, 401)           0           dense_415[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_416 (Dense)                (None, 10)            4020        activation_414[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_415 (Activation)      (None, 10)            0           dense_416[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 611653\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.200215051666\n",
      "Test accuracy: 0.9725\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_417 (Dense)                (None, 512)           401920      dense_input_127[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_416 (Activation)      (None, 512)           0           dense_417[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_418 (Dense)                (None, 451)           231363      activation_416[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_417 (Activation)      (None, 451)           0           dense_418[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_419 (Dense)                (None, 10)            4520        activation_417[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_418 (Activation)      (None, 10)            0           dense_419[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 637803\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.228058558535\n",
      "Test accuracy: 0.966071428571\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_420 (Dense)                (None, 512)           401920      dense_input_128[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_419 (Activation)      (None, 512)           0           dense_420[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_421 (Dense)                (None, 501)           257013      activation_419[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_420 (Activation)      (None, 501)           0           dense_421[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_422 (Dense)                (None, 10)            5020        activation_420[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_421 (Activation)      (None, 10)            0           dense_422[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 663953\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.209602774055\n",
      "Test accuracy: 0.967142857143\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_423 (Dense)                (None, 512)           401920      dense_input_129[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_422 (Activation)      (None, 512)           0           dense_423[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_424 (Dense)                (None, 551)           282663      activation_422[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_423 (Activation)      (None, 551)           0           dense_424[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_425 (Dense)                (None, 10)            5520        activation_423[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_424 (Activation)      (None, 10)            0           dense_425[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 690103\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.189352741582\n",
      "Test accuracy: 0.972738095238\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_426 (Dense)                (None, 512)           401920      dense_input_130[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_425 (Activation)      (None, 512)           0           dense_426[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_427 (Dense)                (None, 601)           308313      activation_425[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_426 (Activation)      (None, 601)           0           dense_427[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_428 (Dense)                (None, 10)            6020        activation_426[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_427 (Activation)      (None, 10)            0           dense_428[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 716253\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.217262318576\n",
      "Test accuracy: 0.969404761905\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_429 (Dense)                (None, 512)           401920      dense_input_131[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_428 (Activation)      (None, 512)           0           dense_429[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_430 (Dense)                (None, 651)           333963      activation_428[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_429 (Activation)      (None, 651)           0           dense_430[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_431 (Dense)                (None, 10)            6520        activation_429[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_430 (Activation)      (None, 10)            0           dense_431[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 742403\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.184706949293\n",
      "Test accuracy: 0.97369047619\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_432 (Dense)                (None, 512)           401920      dense_input_132[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_431 (Activation)      (None, 512)           0           dense_432[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_433 (Dense)                (None, 701)           359613      activation_431[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_432 (Activation)      (None, 701)           0           dense_433[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_434 (Dense)                (None, 10)            7020        activation_432[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_433 (Activation)      (None, 10)            0           dense_434[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 768553\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.216654102507\n",
      "Test accuracy: 0.972142857143\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_435 (Dense)                (None, 512)           401920      dense_input_133[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_434 (Activation)      (None, 512)           0           dense_435[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_436 (Dense)                (None, 751)           385263      activation_434[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_435 (Activation)      (None, 751)           0           dense_436[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_437 (Dense)                (None, 10)            7520        activation_435[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_436 (Activation)      (None, 10)            0           dense_437[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 794703\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.213982076992\n",
      "Test accuracy: 0.970833333333\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_438 (Dense)                (None, 512)           401920      dense_input_134[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_437 (Activation)      (None, 512)           0           dense_438[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_439 (Dense)                (None, 801)           410913      activation_437[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_438 (Activation)      (None, 801)           0           dense_439[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_440 (Dense)                (None, 10)            8020        activation_438[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_439 (Activation)      (None, 10)            0           dense_440[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 820853\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.246612123008\n",
      "Test accuracy: 0.969285714286\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_441 (Dense)                (None, 512)           401920      dense_input_135[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_440 (Activation)      (None, 512)           0           dense_441[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_442 (Dense)                (None, 851)           436563      activation_440[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_441 (Activation)      (None, 851)           0           dense_442[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_443 (Dense)                (None, 10)            8520        activation_441[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_442 (Activation)      (None, 10)            0           dense_443[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 847003\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.29636946937\n",
      "Test accuracy: 0.962976190476\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_444 (Dense)                (None, 512)           401920      dense_input_136[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_443 (Activation)      (None, 512)           0           dense_444[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_445 (Dense)                (None, 901)           462213      activation_443[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_444 (Activation)      (None, 901)           0           dense_445[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_446 (Dense)                (None, 10)            9020        activation_444[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_445 (Activation)      (None, 10)            0           dense_446[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 873153\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.268897663188\n",
      "Test accuracy: 0.966071428571\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_447 (Dense)                (None, 512)           401920      dense_input_137[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_446 (Activation)      (None, 512)           0           dense_447[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_448 (Dense)                (None, 951)           487863      activation_446[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_447 (Activation)      (None, 951)           0           dense_448[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_449 (Dense)                (None, 10)            9520        activation_447[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_448 (Activation)      (None, 10)            0           dense_449[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 899303\n",
      "____________________________________________________________________________________________________\n",
      "Test score: 0.233697802272\n",
      "Test accuracy: 0.968452380952\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 20\n",
    "\n",
    "accuracy = []\n",
    "\n",
    "for deep in range(1, 1000, 50):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(784,)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    \n",
    "    model.add(Dense(deep))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, Y_train,\n",
    "                        batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                        verbose=0, validation_data=(X_test, Y_test))\n",
    "    score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    accuracy.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как размерность выразима через размерность матриц, получаем линейную зависимость от размера слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3fdd8fcf98>]"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAF5CAYAAABEPIrHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJKlSDCeSJKlSDCeSJKlS/j/JtKjDgO2FSwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3fdd8fcf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel(\"Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot([i for i in range(1, 1000, 50)], accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Автоенкодеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "0s - loss: -1.8760e+01 - acc: 0.0047\n",
      "Epoch 2/30\n",
      "0s - loss: 8.9985 - acc: 0.0099\n",
      "Epoch 3/30\n",
      "0s - loss: 36.1823 - acc: 0.0131\n",
      "Epoch 4/30\n",
      "0s - loss: 86.0980 - acc: 0.0159\n",
      "Epoch 5/30\n",
      "0s - loss: 30.2850 - acc: 0.0229\n",
      "Epoch 6/30\n",
      "0s - loss: 42.5165 - acc: 0.0232\n",
      "Epoch 7/30\n",
      "0s - loss: 36.8563 - acc: 0.0213\n",
      "Epoch 8/30\n",
      "0s - loss: 25.2606 - acc: 0.0213\n",
      "Epoch 9/30\n",
      "0s - loss: 8.5517 - acc: 0.0268\n",
      "Epoch 10/30\n",
      "0s - loss: -2.4929e+01 - acc: 0.0313\n",
      "Epoch 11/30\n",
      "0s - loss: -3.4480e+01 - acc: 0.0342\n",
      "Epoch 12/30\n",
      "0s - loss: -2.9874e+01 - acc: 0.0320\n",
      "Epoch 13/30\n",
      "0s - loss: -1.7512e+01 - acc: 0.0321\n",
      "Epoch 14/30\n",
      "0s - loss: 39.4318 - acc: 0.0341\n",
      "Epoch 15/30\n",
      "0s - loss: 10.6014 - acc: 0.0362\n",
      "Epoch 16/30\n",
      "0s - loss: -1.4070e+01 - acc: 0.0404\n",
      "Epoch 17/30\n",
      "0s - loss: -2.3448e+00 - acc: 0.0441\n",
      "Epoch 18/30\n",
      "0s - loss: -3.5327e+01 - acc: 0.0460\n",
      "Epoch 19/30\n",
      "0s - loss: -6.2744e+01 - acc: 0.0476\n",
      "Epoch 20/30\n",
      "0s - loss: 30.6569 - acc: 0.0475\n",
      "Epoch 21/30\n",
      "0s - loss: 42.4920 - acc: 0.0471\n",
      "Epoch 22/30\n",
      "0s - loss: -6.4521e+00 - acc: 0.0464\n",
      "Epoch 23/30\n",
      "0s - loss: -4.7964e+01 - acc: 0.0460\n",
      "Epoch 24/30\n",
      "0s - loss: -6.1046e+01 - acc: 0.0460\n",
      "Epoch 25/30\n",
      "0s - loss: -5.3957e+01 - acc: 0.0454\n",
      "Epoch 26/30\n",
      "0s - loss: -6.6971e+01 - acc: 0.0455\n",
      "Epoch 27/30\n",
      "0s - loss: 29.3512 - acc: 0.0445\n",
      "Epoch 28/30\n",
      "0s - loss: 38.3197 - acc: 0.0456\n",
      "Epoch 29/30\n",
      "0s - loss: 9.1407 - acc: 0.0463\n",
      "Epoch 30/30\n",
      "0s - loss: -2.8059e+01 - acc: 0.0447\n",
      "Epoch 1/30\n",
      "1s - loss: -3.7734e+01 - acc: 0.0039\n",
      "Epoch 2/30\n",
      "0s - loss: 64.5248 - acc: 0.0062\n",
      "Epoch 3/30\n",
      "0s - loss: 54.4021 - acc: 0.0032\n",
      "Epoch 4/30\n",
      "0s - loss: -1.1743e+02 - acc: 0.0038\n",
      "Epoch 5/30\n",
      "0s - loss: -2.3894e+02 - acc: 0.0060\n",
      "Epoch 6/30\n",
      "0s - loss: -3.9904e+02 - acc: 0.0157\n",
      "Epoch 7/30\n",
      "0s - loss: 235.7727 - acc: 0.0125\n",
      "Epoch 8/30\n",
      "0s - loss: 62.0527 - acc: 0.0018\n",
      "Epoch 9/30\n",
      "0s - loss: -3.3803e+01 - acc: 0.0015\n",
      "Epoch 10/30\n",
      "0s - loss: -1.2084e+02 - acc: 0.0012\n",
      "Epoch 11/30\n",
      "0s - loss: -2.7432e+02 - acc: 0.0011\n",
      "Epoch 12/30\n",
      "0s - loss: -9.4079e+01 - acc: 0.0026\n",
      "Epoch 13/30\n",
      "0s - loss: 169.4162 - acc: 0.0050\n",
      "Epoch 14/30\n",
      "0s - loss: -2.7384e+00 - acc: 0.0051\n",
      "Epoch 15/30\n",
      "0s - loss: -6.8947e+01 - acc: 0.0063\n",
      "Epoch 16/30\n",
      "0s - loss: -1.7235e+02 - acc: 0.0082\n",
      "Epoch 17/30\n",
      "0s - loss: -3.0852e+02 - acc: 0.0119\n",
      "Epoch 18/30\n",
      "0s - loss: 113.2388 - acc: 0.0111\n",
      "Epoch 19/30\n",
      "0s - loss: 103.5098 - acc: 0.0025\n",
      "Epoch 20/30\n",
      "0s - loss: -2.4313e+01 - acc: 0.0019\n",
      "Epoch 21/30\n",
      "0s - loss: -7.8859e+01 - acc: 0.0025\n",
      "Epoch 22/30\n",
      "0s - loss: -1.3575e+02 - acc: 0.0021\n",
      "Epoch 23/30\n",
      "0s - loss: -1.1694e+02 - acc: 0.0042\n",
      "Epoch 24/30\n",
      "0s - loss: 25.2713 - acc: 0.0101\n",
      "Epoch 25/30\n",
      "0s - loss: 49.6475 - acc: 0.0181\n",
      "Epoch 26/30\n",
      "0s - loss: -4.9518e+01 - acc: 0.0193\n",
      "Epoch 27/30\n",
      "0s - loss: -1.0920e+02 - acc: 0.0168\n",
      "Epoch 28/30\n",
      "0s - loss: -1.7179e+02 - acc: 0.0139\n",
      "Epoch 29/30\n",
      "0s - loss: -1.8277e+02 - acc: 0.0113\n",
      "Epoch 30/30\n",
      "0s - loss: 112.2627 - acc: 0.0074\n",
      "Epoch 1/30\n",
      "1s - loss: -8.2427e+00 - acc: 0.0026\n",
      "Epoch 2/30\n",
      "0s - loss: 108.2821 - acc: 0.0034\n",
      "Epoch 3/30\n",
      "0s - loss: 1.1539 - acc: 0.0094\n",
      "Epoch 4/30\n",
      "0s - loss: -2.3504e+01 - acc: 0.0036\n",
      "Epoch 5/30\n",
      "0s - loss: -9.7468e+01 - acc: 0.0051\n",
      "Epoch 6/30\n",
      "0s - loss: 20.2396 - acc: 0.0061\n",
      "Epoch 7/30\n",
      "0s - loss: -8.1857e+00 - acc: 0.0041\n",
      "Epoch 8/30\n",
      "0s - loss: -6.2126e+01 - acc: 0.0034\n",
      "Epoch 9/30\n",
      "0s - loss: -7.7940e+01 - acc: 0.0017\n",
      "Epoch 10/30\n",
      "0s - loss: 12.4239 - acc: 0.0014\n",
      "Epoch 11/30\n",
      "0s - loss: -6.6792e+01 - acc: 0.0023\n",
      "Epoch 12/30\n",
      "0s - loss: -4.3097e+01 - acc: 0.0029\n",
      "Epoch 13/30\n",
      "0s - loss: 8.1988 - acc: 0.0033\n",
      "Epoch 14/30\n",
      "0s - loss: -5.6890e+01 - acc: 0.0033\n",
      "Epoch 15/30\n",
      "0s - loss: -1.1222e+02 - acc: 0.0033\n",
      "Epoch 16/30\n",
      "0s - loss: -5.1384e+00 - acc: 0.0019\n",
      "Epoch 17/30\n",
      "0s - loss: -9.4024e+00 - acc: 5.9524e-05\n",
      "Epoch 18/30\n",
      "0s - loss: -3.2347e+01 - acc: 2.9762e-05\n",
      "Epoch 19/30\n",
      "0s - loss: -8.2831e+01 - acc: 8.9286e-05\n",
      "Epoch 20/30\n",
      "0s - loss: -3.2674e+01 - acc: 5.9524e-04\n",
      "Epoch 21/30\n",
      "0s - loss: -4.9298e+00 - acc: 7.1429e-04\n",
      "Epoch 22/30\n",
      "0s - loss: -8.0815e+01 - acc: 0.0013\n",
      "Epoch 23/30\n",
      "0s - loss: -2.6246e+01 - acc: 5.6548e-04\n",
      "Epoch 24/30\n",
      "0s - loss: 3.6156 - acc: 2.9762e-04\n",
      "Epoch 25/30\n",
      "0s - loss: -2.4023e+01 - acc: 2.0833e-04\n",
      "Epoch 26/30\n",
      "0s - loss: -6.2819e+01 - acc: 7.7381e-04\n",
      "Epoch 27/30\n",
      "0s - loss: -1.8582e+02 - acc: 0.0020\n",
      "Epoch 28/30\n",
      "0s - loss: 137.7958 - acc: 0.0038\n",
      "Epoch 29/30\n",
      "0s - loss: -2.7135e+00 - acc: 0.0043\n",
      "Epoch 30/30\n",
      "0s - loss: -4.5070e+01 - acc: 0.0045\n",
      "Epoch 1/30\n",
      "1s - loss: -2.9247e+01 - acc: 0.0019\n",
      "Epoch 2/30\n",
      "1s - loss: -6.7902e+00 - acc: 0.0024\n",
      "Epoch 3/30\n",
      "1s - loss: -3.0096e+01 - acc: 0.0016\n",
      "Epoch 4/30\n",
      "1s - loss: -9.2514e+01 - acc: 8.9286e-05\n",
      "Epoch 5/30\n",
      "1s - loss: 86.6036 - acc: 1.4881e-04\n",
      "Epoch 6/30\n",
      "1s - loss: -1.1995e+02 - acc: 1.1905e-04\n",
      "Epoch 7/30\n",
      "1s - loss: -5.3835e+01 - acc: 5.3571e-04\n",
      "Epoch 8/30\n",
      "1s - loss: 80.4046 - acc: 7.7381e-04\n",
      "Epoch 9/30\n",
      "1s - loss: -1.1632e+02 - acc: 1.1905e-04\n",
      "Epoch 10/30\n",
      "1s - loss: -1.0046e+02 - acc: 1.1905e-04\n",
      "Epoch 11/30\n",
      "1s - loss: 28.0903 - acc: 3.2738e-04\n",
      "Epoch 12/30\n",
      "1s - loss: -4.7172e+01 - acc: 3.2738e-04\n",
      "Epoch 13/30\n",
      "1s - loss: -1.4664e+02 - acc: 0.0014\n",
      "Epoch 14/30\n",
      "1s - loss: -2.5256e+02 - acc: 0.0024\n",
      "Epoch 15/30\n",
      "1s - loss: 107.2625 - acc: 5.9524e-05\n",
      "Epoch 16/30\n",
      "1s - loss: 87.4204 - acc: 1.7857e-04\n",
      "Epoch 17/30\n",
      "1s - loss: -6.7138e+00 - acc: 2.6786e-04\n",
      "Epoch 18/30\n",
      "1s - loss: -1.2412e+02 - acc: 2.9762e-04\n",
      "Epoch 19/30\n",
      "1s - loss: -2.0166e+02 - acc: 3.2738e-04\n",
      "Epoch 20/30\n",
      "1s - loss: -2.6529e+01 - acc: 1.7857e-04\n",
      "Epoch 21/30\n",
      "1s - loss: -4.3418e+01 - acc: 3.8690e-04\n",
      "Epoch 22/30\n",
      "1s - loss: -2.3224e+01 - acc: 2.9762e-04\n",
      "Epoch 23/30\n",
      "1s - loss: -1.3331e+00 - acc: 2.9762e-04\n",
      "Epoch 24/30\n",
      "1s - loss: -1.8949e+02 - acc: 8.9286e-05\n",
      "Epoch 25/30\n",
      "1s - loss: -4.1661e-01 - acc: 1.1905e-04\n",
      "Epoch 26/30\n",
      "1s - loss: -5.1152e+00 - acc: 5.9524e-05\n",
      "Epoch 27/30\n",
      "1s - loss: -1.1367e+02 - acc: 4.7619e-04\n",
      "Epoch 28/30\n",
      "1s - loss: -1.7238e+02 - acc: 0.0018\n",
      "Epoch 29/30\n",
      "1s - loss: -2.0795e+02 - acc: 0.0036\n",
      "Epoch 30/30\n",
      "1s - loss: -2.0039e+02 - acc: 0.0041\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/300\n",
      "33600/33600 [==============================] - 1s - loss: 14.2307 - acc: 0.1145 - val_loss: 14.0640 - val_acc: 0.1273\n",
      "Epoch 2/300\n",
      "33600/33600 [==============================] - 1s - loss: 13.6389 - acc: 0.1535 - val_loss: 13.2896 - val_acc: 0.1755\n",
      "Epoch 3/300\n",
      "33600/33600 [==============================] - 1s - loss: 13.0561 - acc: 0.1900 - val_loss: 13.0173 - val_acc: 0.1924\n",
      "Epoch 4/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9896 - acc: 0.1941 - val_loss: 13.0192 - val_acc: 0.1923\n",
      "Epoch 5/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9866 - acc: 0.1943 - val_loss: 13.0192 - val_acc: 0.1923\n",
      "Epoch 6/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9873 - acc: 0.1942 - val_loss: 13.0668 - val_acc: 0.1892\n",
      "Epoch 7/300\n",
      "33600/33600 [==============================] - 1s - loss: 13.0387 - acc: 0.1910 - val_loss: 12.9943 - val_acc: 0.1938\n",
      "Epoch 8/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9902 - acc: 0.1940 - val_loss: 13.0173 - val_acc: 0.1924\n",
      "Epoch 9/300\n",
      "33600/33600 [==============================] - 1s - loss: 13.0007 - acc: 0.1934 - val_loss: 12.9885 - val_acc: 0.1942\n",
      "Epoch 10/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9693 - acc: 0.1954 - val_loss: 12.9827 - val_acc: 0.1945\n",
      "Epoch 11/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9754 - acc: 0.1950 - val_loss: 13.0710 - val_acc: 0.1890\n",
      "Epoch 12/300\n",
      "33600/33600 [==============================] - 1s - loss: 13.0509 - acc: 0.1903 - val_loss: 13.0173 - val_acc: 0.1924\n",
      "Epoch 13/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9793 - acc: 0.1947 - val_loss: 12.9866 - val_acc: 0.1943\n",
      "Epoch 14/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9681 - acc: 0.1954 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 15/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 16/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 17/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 18/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 19/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 20/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 21/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 22/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 23/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 24/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 25/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 26/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 27/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 28/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 29/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 30/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 31/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 32/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 33/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 34/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 35/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 36/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 37/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 38/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 39/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 40/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 41/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 42/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 43/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 44/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 45/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 46/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 47/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 48/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 49/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 50/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 51/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 52/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 53/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 54/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 55/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 56/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 57/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 58/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 59/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 60/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 61/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 62/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 63/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 64/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 65/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 66/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 67/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 68/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 69/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 70/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 71/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 72/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 73/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 74/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 75/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 76/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 77/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 78/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 79/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 80/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 81/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 82/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 83/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 84/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 85/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 86/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 87/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 88/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 89/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 90/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 91/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 92/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 93/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 94/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 95/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 96/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 97/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 98/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 99/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 100/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 101/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 102/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 103/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 104/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 105/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 106/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 107/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 108/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 109/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 110/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 111/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 112/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 113/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 114/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 115/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 116/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 117/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 118/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 119/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 120/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 121/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 122/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 123/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 124/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 125/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 126/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 127/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 128/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 129/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 130/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 131/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 132/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 133/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 134/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 135/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 136/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 137/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 138/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 139/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 140/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 141/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 142/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 143/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 144/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 145/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 146/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 147/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 148/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 149/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 150/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 151/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 152/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 153/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 154/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 155/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 156/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 157/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 158/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 159/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 160/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 161/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 162/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 163/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 164/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 165/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 166/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 167/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 168/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 169/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 170/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 171/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 172/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 173/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 174/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 175/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 176/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 177/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 178/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 179/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 180/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 181/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 182/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 183/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 184/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 185/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 186/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 187/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 188/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 189/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 190/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 191/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 192/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 193/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 194/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 195/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 196/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 197/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 198/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 199/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 200/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 201/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 202/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 203/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 204/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 205/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 206/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 207/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 208/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 209/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 210/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 211/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 212/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 213/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 214/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 215/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 216/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 217/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 218/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 219/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 220/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 221/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 222/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 223/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 224/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 225/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 226/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 227/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 228/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 229/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 230/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 231/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 232/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 233/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 234/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 235/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 236/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 237/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 238/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 239/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 240/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 241/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 242/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 243/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 244/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 245/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 246/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 247/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 248/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 249/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 250/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 251/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 252/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 253/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 254/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 255/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 256/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 257/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 258/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 259/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 260/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 261/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 262/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 263/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 264/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 265/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 266/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 267/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 268/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 269/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 270/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 271/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 272/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 273/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 274/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 275/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 276/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 277/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 278/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 279/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 280/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 281/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 282/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 283/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 284/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 285/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 286/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 287/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 288/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 289/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 290/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 291/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 292/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 293/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 294/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 295/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 296/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 297/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 298/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 299/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Epoch 300/300\n",
      "33600/33600 [==============================] - 1s - loss: 12.9616 - acc: 0.1958 - val_loss: 12.9923 - val_acc: 0.1939\n",
      "Test score: 12.9923362132\n",
      "Test accuracy: 0.193928571429\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "nb_epoch = 30\n",
    "\n",
    "layers = [\n",
    "    Dense(784, input_shape=(784,)),\n",
    "    Activation('relu')\n",
    "]\n",
    "\n",
    "autoencoder = Dense(784, input_shape=(784,))\n",
    "\n",
    "for num in range(1, 5):\n",
    "    model = Sequential(layers[:len(layers) - 2] + [autoencoder])\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, X_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=2)\n",
    "    \n",
    "    model = Sequential(layers)    \n",
    "    autoencoder = Dense(784)\n",
    "\n",
    "    layers.append(Dense(784))\n",
    "    layers.append(Activation('relu'))\n",
    "    \n",
    "    \n",
    "layers.append(Dense(10))\n",
    "layers.append(Activation('softmax'))\n",
    "\n",
    "model = Sequential(layers)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=Adam(),\n",
    "             metrics=['accuracy'])\n",
    "    \n",
    "model.fit(X_train, Y_train,batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch * 10, verbose=1,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Никакого улучшения или прогресса =("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание***\n",
    "\n",
    "Предложите свою архитектуру полносвязанной сети, которая показывает лучшее качество на mnist. Вы вольны использовать произвольное количество слоев, другие [функции активации](https://en.wikipedia.org/wiki/Activation_function) и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_387 (Dense)                (None, 784)           615440      dense_input_117[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_386 (Activation)      (None, 784)           0           dense_387[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)              (None, 784)           0           activation_386[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_388 (Dense)                (None, 256)           200960      dropout_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_387 (Activation)      (None, 256)           0           dense_388[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_389 (Dense)                (None, 10)            2570        activation_387[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_388 (Activation)      (None, 10)            0           dense_389[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 818970\n",
      "____________________________________________________________________________________________________\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/200\n",
      "33600/33600 [==============================] - 2s - loss: 0.2912 - acc: 0.9151 - val_loss: 0.1763 - val_acc: 0.9504\n",
      "Epoch 2/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.1223 - acc: 0.9643 - val_loss: 0.1508 - val_acc: 0.9573\n",
      "Epoch 3/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0796 - acc: 0.9762 - val_loss: 0.1453 - val_acc: 0.9605\n",
      "Epoch 4/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0614 - acc: 0.9809 - val_loss: 0.1583 - val_acc: 0.9623\n",
      "Epoch 5/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0542 - acc: 0.9840 - val_loss: 0.1646 - val_acc: 0.9630\n",
      "Epoch 6/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0501 - acc: 0.9851 - val_loss: 0.1481 - val_acc: 0.9655\n",
      "Epoch 7/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0375 - acc: 0.9885 - val_loss: 0.1472 - val_acc: 0.9676\n",
      "Epoch 8/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0403 - acc: 0.9895 - val_loss: 0.1799 - val_acc: 0.9651\n",
      "Epoch 9/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0346 - acc: 0.9904 - val_loss: 0.1620 - val_acc: 0.9662\n",
      "Epoch 10/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0338 - acc: 0.9909 - val_loss: 0.1791 - val_acc: 0.9675\n",
      "Epoch 11/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0370 - acc: 0.9909 - val_loss: 0.1698 - val_acc: 0.9686\n",
      "Epoch 12/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0356 - acc: 0.9910 - val_loss: 0.1864 - val_acc: 0.9668\n",
      "Epoch 13/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0301 - acc: 0.9922 - val_loss: 0.1609 - val_acc: 0.9714\n",
      "Epoch 14/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0524 - acc: 0.9874 - val_loss: 0.2210 - val_acc: 0.9638\n",
      "Epoch 15/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0451 - acc: 0.9886 - val_loss: 0.2390 - val_acc: 0.9654\n",
      "Epoch 16/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0314 - acc: 0.9925 - val_loss: 0.1831 - val_acc: 0.9695\n",
      "Epoch 17/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0188 - acc: 0.9951 - val_loss: 0.1684 - val_acc: 0.9729\n",
      "Epoch 18/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0215 - acc: 0.9948 - val_loss: 0.2012 - val_acc: 0.9693\n",
      "Epoch 19/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0239 - acc: 0.9944 - val_loss: 0.2042 - val_acc: 0.9698\n",
      "Epoch 20/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0197 - acc: 0.9951 - val_loss: 0.1890 - val_acc: 0.9710\n",
      "Epoch 21/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0167 - acc: 0.9961 - val_loss: 0.1740 - val_acc: 0.9708\n",
      "Epoch 22/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0251 - acc: 0.9944 - val_loss: 0.1911 - val_acc: 0.9694\n",
      "Epoch 23/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0280 - acc: 0.9927 - val_loss: 0.2496 - val_acc: 0.9640\n",
      "Epoch 24/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0297 - acc: 0.9929 - val_loss: 0.2644 - val_acc: 0.9670\n",
      "Epoch 25/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0360 - acc: 0.9916 - val_loss: 0.2411 - val_acc: 0.9660\n",
      "Epoch 26/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0169 - acc: 0.9957 - val_loss: 0.2085 - val_acc: 0.9711\n",
      "Epoch 27/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0184 - acc: 0.9963 - val_loss: 0.2029 - val_acc: 0.9723\n",
      "Epoch 28/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0168 - acc: 0.9964 - val_loss: 0.2177 - val_acc: 0.9713\n",
      "Epoch 29/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0200 - acc: 0.9960 - val_loss: 0.2390 - val_acc: 0.9683\n",
      "Epoch 30/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0255 - acc: 0.9940 - val_loss: 0.2428 - val_acc: 0.9669\n",
      "Epoch 31/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0382 - acc: 0.9925 - val_loss: 0.2384 - val_acc: 0.9714\n",
      "Epoch 32/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0294 - acc: 0.9949 - val_loss: 0.2199 - val_acc: 0.9729\n",
      "Epoch 33/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0212 - acc: 0.9961 - val_loss: 0.2244 - val_acc: 0.9711\n",
      "Epoch 34/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0190 - acc: 0.9962 - val_loss: 0.2056 - val_acc: 0.9725\n",
      "Epoch 35/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0180 - acc: 0.9960 - val_loss: 0.2142 - val_acc: 0.9724\n",
      "Epoch 36/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0185 - acc: 0.9960 - val_loss: 0.2424 - val_acc: 0.9704\n",
      "Epoch 37/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0230 - acc: 0.9959 - val_loss: 0.2479 - val_acc: 0.9707\n",
      "Epoch 38/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0217 - acc: 0.9959 - val_loss: 0.2331 - val_acc: 0.9729\n",
      "Epoch 39/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0283 - acc: 0.9946 - val_loss: 0.2565 - val_acc: 0.9696\n",
      "Epoch 40/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0212 - acc: 0.9956 - val_loss: 0.2310 - val_acc: 0.9738\n",
      "Epoch 41/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0274 - acc: 0.9959 - val_loss: 0.3114 - val_acc: 0.9661\n",
      "Epoch 42/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0257 - acc: 0.9951 - val_loss: 0.2571 - val_acc: 0.9720\n",
      "Epoch 43/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0233 - acc: 0.9961 - val_loss: 0.2492 - val_acc: 0.9724\n",
      "Epoch 44/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0244 - acc: 0.9960 - val_loss: 0.2377 - val_acc: 0.9742\n",
      "Epoch 45/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0239 - acc: 0.9957 - val_loss: 0.2610 - val_acc: 0.9701\n",
      "Epoch 46/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0236 - acc: 0.9963 - val_loss: 0.2557 - val_acc: 0.9739\n",
      "Epoch 47/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0169 - acc: 0.9969 - val_loss: 0.2874 - val_acc: 0.9686\n",
      "Epoch 48/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0168 - acc: 0.9969 - val_loss: 0.2482 - val_acc: 0.9740\n",
      "Epoch 49/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0169 - acc: 0.9971 - val_loss: 0.2690 - val_acc: 0.9714\n",
      "Epoch 50/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0175 - acc: 0.9969 - val_loss: 0.2703 - val_acc: 0.9720\n",
      "Epoch 51/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0279 - acc: 0.9952 - val_loss: 0.2737 - val_acc: 0.9725\n",
      "Epoch 52/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0308 - acc: 0.9954 - val_loss: 0.2714 - val_acc: 0.9712\n",
      "Epoch 53/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0191 - acc: 0.9970 - val_loss: 0.2724 - val_acc: 0.9730\n",
      "Epoch 54/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0157 - acc: 0.9973 - val_loss: 0.2656 - val_acc: 0.9708\n",
      "Epoch 55/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0304 - acc: 0.9951 - val_loss: 0.2941 - val_acc: 0.9704\n",
      "Epoch 56/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0381 - acc: 0.9944 - val_loss: 0.2844 - val_acc: 0.9723\n",
      "Epoch 57/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0283 - acc: 0.9960 - val_loss: 0.2827 - val_acc: 0.9733\n",
      "Epoch 58/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0209 - acc: 0.9968 - val_loss: 0.2473 - val_acc: 0.9748\n",
      "Epoch 59/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0163 - acc: 0.9977 - val_loss: 0.2810 - val_acc: 0.9713\n",
      "Epoch 60/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0196 - acc: 0.9971 - val_loss: 0.2597 - val_acc: 0.9737\n",
      "Epoch 61/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0176 - acc: 0.9974 - val_loss: 0.2661 - val_acc: 0.9749\n",
      "Epoch 62/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0171 - acc: 0.9974 - val_loss: 0.2968 - val_acc: 0.9725\n",
      "Epoch 63/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0150 - acc: 0.9978 - val_loss: 0.2876 - val_acc: 0.9721\n",
      "Epoch 64/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0155 - acc: 0.9976 - val_loss: 0.3023 - val_acc: 0.9720\n",
      "Epoch 65/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0228 - acc: 0.9968 - val_loss: 0.2970 - val_acc: 0.9708\n",
      "Epoch 66/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0209 - acc: 0.9972 - val_loss: 0.2841 - val_acc: 0.9732\n",
      "Epoch 67/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0204 - acc: 0.9969 - val_loss: 0.2584 - val_acc: 0.9771\n",
      "Epoch 68/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0256 - acc: 0.9966 - val_loss: 0.2640 - val_acc: 0.9748\n",
      "Epoch 69/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0223 - acc: 0.9972 - val_loss: 0.2860 - val_acc: 0.9744\n",
      "Epoch 70/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0196 - acc: 0.9973 - val_loss: 0.2901 - val_acc: 0.9725\n",
      "Epoch 71/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0246 - acc: 0.9968 - val_loss: 0.3050 - val_acc: 0.9729\n",
      "Epoch 72/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0229 - acc: 0.9969 - val_loss: 0.2999 - val_acc: 0.9720\n",
      "Epoch 73/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0188 - acc: 0.9973 - val_loss: 0.2808 - val_acc: 0.9738\n",
      "Epoch 74/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0126 - acc: 0.9980 - val_loss: 0.2932 - val_acc: 0.9749\n",
      "Epoch 75/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0257 - acc: 0.9968 - val_loss: 0.3035 - val_acc: 0.9737\n",
      "Epoch 76/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0310 - acc: 0.9960 - val_loss: 0.3066 - val_acc: 0.9732\n",
      "Epoch 77/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0289 - acc: 0.9964 - val_loss: 0.3214 - val_acc: 0.9724\n",
      "Epoch 78/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0260 - acc: 0.9962 - val_loss: 0.3111 - val_acc: 0.9732\n",
      "Epoch 79/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0248 - acc: 0.9972 - val_loss: 0.3309 - val_acc: 0.9724\n",
      "Epoch 80/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0292 - acc: 0.9966 - val_loss: 0.3599 - val_acc: 0.9707\n",
      "Epoch 81/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0235 - acc: 0.9969 - val_loss: 0.3074 - val_acc: 0.9749\n",
      "Epoch 82/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0313 - acc: 0.9965 - val_loss: 0.3382 - val_acc: 0.9713\n",
      "Epoch 83/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0312 - acc: 0.9963 - val_loss: 0.3675 - val_acc: 0.9704\n",
      "Epoch 84/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0273 - acc: 0.9970 - val_loss: 0.3382 - val_acc: 0.9721\n",
      "Epoch 85/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0247 - acc: 0.9973 - val_loss: 0.3634 - val_acc: 0.9704\n",
      "Epoch 86/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0249 - acc: 0.9973 - val_loss: 0.3342 - val_acc: 0.9718\n",
      "Epoch 87/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0211 - acc: 0.9977 - val_loss: 0.3258 - val_acc: 0.9726\n",
      "Epoch 88/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0235 - acc: 0.9975 - val_loss: 0.3101 - val_acc: 0.9748\n",
      "Epoch 89/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0236 - acc: 0.9973 - val_loss: 0.3265 - val_acc: 0.9737\n",
      "Epoch 90/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0250 - acc: 0.9975 - val_loss: 0.3719 - val_acc: 0.9701\n",
      "Epoch 91/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0304 - acc: 0.9969 - val_loss: 0.3428 - val_acc: 0.9730\n",
      "Epoch 92/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0240 - acc: 0.9972 - val_loss: 0.3124 - val_acc: 0.9751\n",
      "Epoch 93/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0287 - acc: 0.9972 - val_loss: 0.3541 - val_acc: 0.9715\n",
      "Epoch 94/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0268 - acc: 0.9969 - val_loss: 0.3138 - val_acc: 0.9751\n",
      "Epoch 95/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0240 - acc: 0.9971 - val_loss: 0.3490 - val_acc: 0.9732\n",
      "Epoch 96/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0326 - acc: 0.9968 - val_loss: 0.3152 - val_acc: 0.9751\n",
      "Epoch 97/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0306 - acc: 0.9968 - val_loss: 0.3191 - val_acc: 0.9740\n",
      "Epoch 98/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0270 - acc: 0.9973 - val_loss: 0.3604 - val_acc: 0.9717\n",
      "Epoch 99/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0370 - acc: 0.9965 - val_loss: 0.3430 - val_acc: 0.9718\n",
      "Epoch 100/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0301 - acc: 0.9970 - val_loss: 0.3387 - val_acc: 0.9738\n",
      "Epoch 101/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0272 - acc: 0.9972 - val_loss: 0.3198 - val_acc: 0.9758\n",
      "Epoch 102/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0245 - acc: 0.9977 - val_loss: 0.3371 - val_acc: 0.9732\n",
      "Epoch 103/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0250 - acc: 0.9973 - val_loss: 0.3128 - val_acc: 0.9748\n",
      "Epoch 104/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0192 - acc: 0.9979 - val_loss: 0.3537 - val_acc: 0.9730\n",
      "Epoch 105/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0318 - acc: 0.9968 - val_loss: 0.3417 - val_acc: 0.9746\n",
      "Epoch 106/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0248 - acc: 0.9977 - val_loss: 0.3525 - val_acc: 0.9729\n",
      "Epoch 107/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0276 - acc: 0.9974 - val_loss: 0.3361 - val_acc: 0.9748\n",
      "Epoch 108/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0276 - acc: 0.9973 - val_loss: 0.3464 - val_acc: 0.9736\n",
      "Epoch 109/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0273 - acc: 0.9974 - val_loss: 0.3359 - val_acc: 0.9744\n",
      "Epoch 110/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0441 - acc: 0.9960 - val_loss: 0.3743 - val_acc: 0.9719\n",
      "Epoch 111/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0382 - acc: 0.9964 - val_loss: 0.3469 - val_acc: 0.9743\n",
      "Epoch 112/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0358 - acc: 0.9968 - val_loss: 0.3960 - val_acc: 0.9704\n",
      "Epoch 113/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0300 - acc: 0.9973 - val_loss: 0.3441 - val_acc: 0.9740\n",
      "Epoch 114/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0361 - acc: 0.9969 - val_loss: 0.3535 - val_acc: 0.9740\n",
      "Epoch 115/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0435 - acc: 0.9959 - val_loss: 0.3844 - val_acc: 0.9706\n",
      "Epoch 116/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0327 - acc: 0.9971 - val_loss: 0.3832 - val_acc: 0.9721\n",
      "Epoch 117/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0462 - acc: 0.9960 - val_loss: 0.3588 - val_acc: 0.9733\n",
      "Epoch 118/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0438 - acc: 0.9962 - val_loss: 0.3779 - val_acc: 0.9729\n",
      "Epoch 119/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0270 - acc: 0.9975 - val_loss: 0.3685 - val_acc: 0.9739\n",
      "Epoch 120/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0332 - acc: 0.9971 - val_loss: 0.3617 - val_acc: 0.9742\n",
      "Epoch 121/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0273 - acc: 0.9976 - val_loss: 0.3664 - val_acc: 0.9735\n",
      "Epoch 122/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0437 - acc: 0.9963 - val_loss: 0.4023 - val_acc: 0.9713\n",
      "Epoch 123/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0422 - acc: 0.9962 - val_loss: 0.3651 - val_acc: 0.9738\n",
      "Epoch 124/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0345 - acc: 0.9968 - val_loss: 0.3430 - val_acc: 0.9752\n",
      "Epoch 125/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0242 - acc: 0.9977 - val_loss: 0.3460 - val_acc: 0.9752\n",
      "Epoch 126/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0342 - acc: 0.9971 - val_loss: 0.3643 - val_acc: 0.9730\n",
      "Epoch 127/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0320 - acc: 0.9974 - val_loss: 0.3646 - val_acc: 0.9745\n",
      "Epoch 128/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0274 - acc: 0.9976 - val_loss: 0.3802 - val_acc: 0.9730\n",
      "Epoch 129/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0251 - acc: 0.9977 - val_loss: 0.3409 - val_acc: 0.9760\n",
      "Epoch 130/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0242 - acc: 0.9977 - val_loss: 0.3502 - val_acc: 0.9743\n",
      "Epoch 131/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0248 - acc: 0.9976 - val_loss: 0.3583 - val_acc: 0.9749\n",
      "Epoch 132/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0298 - acc: 0.9973 - val_loss: 0.3700 - val_acc: 0.9743\n",
      "Epoch 133/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0424 - acc: 0.9964 - val_loss: 0.3258 - val_acc: 0.9762\n",
      "Epoch 134/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0414 - acc: 0.9968 - val_loss: 0.3229 - val_acc: 0.9768\n",
      "Epoch 135/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0395 - acc: 0.9968 - val_loss: 0.3461 - val_acc: 0.9755\n",
      "Epoch 136/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0372 - acc: 0.9969 - val_loss: 0.3313 - val_acc: 0.9764\n",
      "Epoch 137/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0278 - acc: 0.9977 - val_loss: 0.3708 - val_acc: 0.9744\n",
      "Epoch 138/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0271 - acc: 0.9976 - val_loss: 0.3478 - val_acc: 0.9760\n",
      "Epoch 139/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0436 - acc: 0.9966 - val_loss: 0.3626 - val_acc: 0.9750\n",
      "Epoch 140/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0336 - acc: 0.9972 - val_loss: 0.3334 - val_acc: 0.9760\n",
      "Epoch 141/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0374 - acc: 0.9971 - val_loss: 0.3384 - val_acc: 0.9756\n",
      "Epoch 142/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0384 - acc: 0.9968 - val_loss: 0.3460 - val_acc: 0.9752\n",
      "Epoch 143/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0366 - acc: 0.9972 - val_loss: 0.3728 - val_acc: 0.9748\n",
      "Epoch 144/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0351 - acc: 0.9972 - val_loss: 0.3411 - val_acc: 0.9765\n",
      "Epoch 145/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0300 - acc: 0.9974 - val_loss: 0.3853 - val_acc: 0.9731\n",
      "Epoch 146/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0469 - acc: 0.9963 - val_loss: 0.3763 - val_acc: 0.9738\n",
      "Epoch 147/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0489 - acc: 0.9961 - val_loss: 0.3870 - val_acc: 0.9733\n",
      "Epoch 148/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0371 - acc: 0.9971 - val_loss: 0.3656 - val_acc: 0.9743\n",
      "Epoch 149/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0351 - acc: 0.9972 - val_loss: 0.3406 - val_acc: 0.9763\n",
      "Epoch 150/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0265 - acc: 0.9976 - val_loss: 0.3616 - val_acc: 0.9749\n",
      "Epoch 151/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0441 - acc: 0.9964 - val_loss: 0.3759 - val_acc: 0.9738\n",
      "Epoch 152/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0355 - acc: 0.9973 - val_loss: 0.4268 - val_acc: 0.9712\n",
      "Epoch 153/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0504 - acc: 0.9963 - val_loss: 0.3829 - val_acc: 0.9736\n",
      "Epoch 154/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0496 - acc: 0.9963 - val_loss: 0.3654 - val_acc: 0.9745\n",
      "Epoch 155/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0381 - acc: 0.9971 - val_loss: 0.3804 - val_acc: 0.9735\n",
      "Epoch 156/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0543 - acc: 0.9960 - val_loss: 0.4045 - val_acc: 0.9724\n",
      "Epoch 157/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0429 - acc: 0.9966 - val_loss: 0.3765 - val_acc: 0.9740\n",
      "Epoch 158/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0470 - acc: 0.9963 - val_loss: 0.3719 - val_acc: 0.9739\n",
      "Epoch 159/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0586 - acc: 0.9957 - val_loss: 0.4229 - val_acc: 0.9717\n",
      "Epoch 160/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0498 - acc: 0.9963 - val_loss: 0.3893 - val_acc: 0.9727\n",
      "Epoch 161/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0387 - acc: 0.9970 - val_loss: 0.3765 - val_acc: 0.9744\n",
      "Epoch 162/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0418 - acc: 0.9968 - val_loss: 0.3421 - val_acc: 0.9771\n",
      "Epoch 163/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0397 - acc: 0.9968 - val_loss: 0.3729 - val_acc: 0.9748\n",
      "Epoch 164/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0485 - acc: 0.9966 - val_loss: 0.3980 - val_acc: 0.9726\n",
      "Epoch 165/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0397 - acc: 0.9970 - val_loss: 0.3659 - val_acc: 0.9754\n",
      "Epoch 166/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0402 - acc: 0.9972 - val_loss: 0.3816 - val_acc: 0.9742\n",
      "Epoch 167/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0444 - acc: 0.9965 - val_loss: 0.4190 - val_acc: 0.9711\n",
      "Epoch 168/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0385 - acc: 0.9970 - val_loss: 0.4018 - val_acc: 0.9730\n",
      "Epoch 169/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0353 - acc: 0.9975 - val_loss: 0.3915 - val_acc: 0.9738\n",
      "Epoch 170/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0292 - acc: 0.9977 - val_loss: 0.3942 - val_acc: 0.9736\n",
      "Epoch 171/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0307 - acc: 0.9977 - val_loss: 0.3769 - val_acc: 0.9757\n",
      "Epoch 172/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0238 - acc: 0.9981 - val_loss: 0.3921 - val_acc: 0.9739\n",
      "Epoch 173/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0313 - acc: 0.9976 - val_loss: 0.3570 - val_acc: 0.9755\n",
      "Epoch 174/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0387 - acc: 0.9972 - val_loss: 0.3776 - val_acc: 0.9744\n",
      "Epoch 175/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0833 - acc: 0.9942 - val_loss: 0.4463 - val_acc: 0.9707\n",
      "Epoch 176/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0771 - acc: 0.9944 - val_loss: 0.3692 - val_acc: 0.9750\n",
      "Epoch 177/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0501 - acc: 0.9963 - val_loss: 0.3312 - val_acc: 0.9777\n",
      "Epoch 178/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0347 - acc: 0.9973 - val_loss: 0.3840 - val_acc: 0.9742\n",
      "Epoch 179/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0513 - acc: 0.9961 - val_loss: 0.3981 - val_acc: 0.9736\n",
      "Epoch 180/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0516 - acc: 0.9963 - val_loss: 0.4007 - val_acc: 0.9733\n",
      "Epoch 181/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0450 - acc: 0.9968 - val_loss: 0.4040 - val_acc: 0.9730\n",
      "Epoch 182/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0436 - acc: 0.9967 - val_loss: 0.3589 - val_acc: 0.9752\n",
      "Epoch 183/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0399 - acc: 0.9972 - val_loss: 0.3498 - val_acc: 0.9765\n",
      "Epoch 184/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0333 - acc: 0.9976 - val_loss: 0.3704 - val_acc: 0.9756\n",
      "Epoch 185/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0422 - acc: 0.9968 - val_loss: 0.3843 - val_acc: 0.9745\n",
      "Epoch 186/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0653 - acc: 0.9954 - val_loss: 0.4017 - val_acc: 0.9735\n",
      "Epoch 187/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0592 - acc: 0.9959 - val_loss: 0.4413 - val_acc: 0.9710\n",
      "Epoch 188/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0774 - acc: 0.9943 - val_loss: 0.3999 - val_acc: 0.9720\n",
      "Epoch 189/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0607 - acc: 0.9957 - val_loss: 0.4204 - val_acc: 0.9715\n",
      "Epoch 190/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0567 - acc: 0.9958 - val_loss: 0.4330 - val_acc: 0.9710\n",
      "Epoch 191/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0462 - acc: 0.9966 - val_loss: 0.4431 - val_acc: 0.9707\n",
      "Epoch 192/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0920 - acc: 0.9936 - val_loss: 0.4153 - val_acc: 0.9714\n",
      "Epoch 193/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0461 - acc: 0.9967 - val_loss: 0.4006 - val_acc: 0.9727\n",
      "Epoch 194/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0684 - acc: 0.9952 - val_loss: 0.4486 - val_acc: 0.9698\n",
      "Epoch 195/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0690 - acc: 0.9950 - val_loss: 0.4086 - val_acc: 0.9725\n",
      "Epoch 196/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0529 - acc: 0.9963 - val_loss: 0.4089 - val_acc: 0.9733\n",
      "Epoch 197/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0547 - acc: 0.9962 - val_loss: 0.4470 - val_acc: 0.9708\n",
      "Epoch 198/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0717 - acc: 0.9952 - val_loss: 0.4351 - val_acc: 0.9712\n",
      "Epoch 199/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0478 - acc: 0.9968 - val_loss: 0.4002 - val_acc: 0.9735\n",
      "Epoch 200/200\n",
      "33600/33600 [==============================] - 1s - loss: 0.0468 - acc: 0.9967 - val_loss: 0.4218 - val_acc: 0.9720\n",
      "Test score: 0.421841754811\n",
      "Test accuracy: 0.972023809524\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 200\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(784, input_shape=(784,)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(p=0.2))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
